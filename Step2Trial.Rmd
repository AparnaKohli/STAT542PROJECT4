---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


# System I: recommendation based on genres.

Read in the data and set appropriate column names . First, we start, with the ratings data and then go to the movies data. We first bring the data into a readable format.

```{r}

library(reshape2)
library(tidyr)

myurl = "https://liangfgithub.github.io/MovieData/"
movies = readLines(paste0(myurl, 'movies.dat?raw=true'))
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)
movies$Title = iconv(movies$Title, "latin1", "UTF-8")


# extract year
movies$Year = as.numeric(unlist(
    lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))

myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'), 
                   sep = ':',
                   colClasses = c('integer', 'NULL'), 
                   header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
Ratings_Aggregate = aggregate(ratings,
                              list(ratings$MovieID),
                              mean)


Number_Of_Ratings = plyr::count(ratings, vars = "MovieID")
colnames(Number_Of_Ratings) = c("MovieID", "Number.of.Ratings")

movies = transform(merge(movies,Number_Of_Ratings,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)
movies = transform(merge(movies,Ratings_Aggregate,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

movies = subset(movies, select = c("MovieID.x", "Title", "Genres", "Year", "Number.of.Ratings", "Rating"))
names(movies)[names(movies) == "MovieID.x"] <- "MovieID"

movies = separate(data = movies, col = Genres, into = c('Genre.1', 'Genre.2', 'Genre.3', 'Genre.4', 'Genre.5', 'Genre.6'), sep = "\\|")





```

We then aggregate the data to find an average of the ratings of each movie in the ratings dataset. Next, we calculate the number of ratings for every movie. Clearly, if the number of ratings are few, then the average rating becomes less trustworthy. We then create a final dataset where at a movie level (title and movie ID), we provide data on the average rating across all users, number of ratings, year, genres. We split the data on genres into multiple columns, to better filter out movies which belong to multiple genres. 



#First algorithm: Highest Rated Movies from the Same Genre

We first propose recommending the highest rated movies from the user selected genres. To do this, we first take user input on their favourite genres. Users are able to select 1 or more genres. For each user selected genre, we filter the dataset to find movies from the same genre. We do not give any importance to the fact whether the genre mentioned was Genre 1 or Genre 6 of the movie. Since every movie should have a decent number of reviews to avoid bias, we filter out movies which have <100 reviews. We then sort the dataset on the basis of ratings and select the top 20 movies. 



```{r}
highest_rated = function(genre){
    relevant_movies = subset(movies, movies$Genre.1==genre | movies$`Genre.2`==genre|movies$`Genre.3`==genre|movies$`Genre.4`==genre|movies$`Genre.5`==genre|movies$`Genre.6`==genre)
    #filter relevant movies
    relevant_movies = subset(relevant_movies, relevant_movies$`Number.of.Ratings`>=100)
    
    #Sort in descending order of ratings 
    newdata = relevant_movies[order(-relevant_movies$Rating),][1:20,]
    #Pick the 1st 20
    #final_list[,i] = newdata$Title[1:20]
    newdata = newdata[1:20,]
    return(newdata)
    
  }
  


```


#Algorithm 2 : IMDB Top movies from each genre

Here we find the top 20 movies in each genre using IMDBs weighted average formula 

$ (WR) = (v/(v+m)) R + (m/(v+m))C $
where WR is the weighted rating
-v is the number of votes for the movie
-m is the minimum number of votes required to be in this list
-R is the average rating of the movie
-C is the mean rating across the report

For our purpose we have morphed it as follows:
-v is the number of reviews a movie received
-m is the minimum number of votes required to be in this list (>= 95th centile of the number of ratings column)
-R is average rating
-C is the mean rating of the movies shortlisted after applying filter of m




```{r}
# Minimum number of reviews a movie should have received
m = quantile(movies$`Number of Ratings`, 0.95)

# Subset of the dataset which has movies with at least m reviews

top_X = subset(movies, movies$`Number of Ratings`>=m)

#Mean rating of the movies shortlisted
C = mean(top_X$Rating)

movies$imdbRating = ((movies$`Number.of.Ratings` * movies$Rating) /(movies$`Number.of.Ratings`+m)) + ((m * C)/(movies$`Number.of.Ratings`+m))

top_20 = function(genre){
  
  
  top_20_list = matrix(NA, nrow = 20, ncol = length(genre_list))
  
  
    
    #Select the first genre from the list
    
    genre = genre_list[i]
    finaldata = movies[order(-movies$imdbRating),]
    top_20_list[,i] = finaldata$Title[1:20]
    
    return(top_20_list)
  
  }
  
  





```





##System 2

We explored following algorithms for building the recommendation system (the algos are ):

* UBCF (User-based collaborative filtering )
* IBCF (Item-based collaborative filtering)
* POPULAR (Popular items)

Brief Description of above Algos:

##### User-based collaborative filtering:

User-Based Collaborative Filtering is a technique used to predict the items that a user might like on the basis of ratings given to that item by the other users who have similar taste with that of the target user. It uses this logic and recommends items by finding similar users to the active user (to whom we are trying to recommend a movie). We use user-based Nearest Neighbor algorithm in following way:

1. Find the K-nearest neighbors (KNN) to the user a, using a similarity function to measure the distance between each pair of users.
2. Predict the rating that user a will give to all items the k neighbors have consumed but a has not. We Look for the item j with the best predicted rating.

In other words, we are creating a User-Item Matrix, predicting the ratings on items the active user has not see, based on the other similar users. This technique is memory-based.

##### Item-based collaborative filtering:

In this technique, explore the relationship between the pair of items (the user who bought Y, also bought Z). We find the missing rating with the help of the ratings given to the other items by the user.We can divide IBCF into two main sub tasks:

1. Calculating similarity among the items using measures like Cosine-Based Similarity or 1-Jaccard distance
2. Calculating the prediction. This includes Weighted Sum and Regression

The difference between UBCF and this method is that, in this case, we directly pre-calculate the similarity between the co-rated items, skipping K-neighborhood search.


#### Code explanation and techincal details:

We used *UBCF*, *IBCF* methods from Recommenderlab library as follows:

> Recommender(getData(f, "train"), method = 'UBCF', parameter = list(normalize = 'z-score', method = 'cosine', nn = 25))

Here, we are getting the data for each recommender algo from *evaluationScheme* method.

Technical Details for the above methods (*UBCF*, *IBCF*):

* we normalized the matrix beforehand to reduce the individual rating bias by row centering the data. We used Z-score normalization that subtracting from each available rating the mean of the ratings of that user (row) and also divides by the standard deviation of the row/column.
* Used cosine as the similarity measure for identifying similarities between users
* In case of UBCF, We took nearest neighbors as 25. Given the large amount of data, having higher neighbors could help in better predictions. IBCF doesn't need any nearest neighbor param as part of its calculation
* Nearest neighborhood size 3
* We havenâ€™t used prediction is based on a "weighted average"


## Running the collaborative recommendation system Algorithms

We ran the algos for 10 iterations and recorded the error in the graph as displayed







```{r}

library(recommenderlab)
library(Matrix)
library(plyr)
library(ggplot2)
library(reshape2)
library(tidyverse)


myurl = "https://liangfgithub.github.io/MovieData/"
movies = readLines(paste0(myurl, 'movies.dat?raw=true'))
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)
movies$Title = iconv(movies$Title, "latin1", "UTF-8")


# extract year
movies$Year = as.numeric(unlist(
    lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))



# Splitting the dataset into training and test. We then convert the training dataset into a real ratings matrix. We train the model and calculate the prediction accuracy on the held out dataset. 
library(recommenderlab)
library(Matrix)




myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'), 
                   sep = ':',
                   colClasses = c('integer', 'NULL'), 
                   header = FALSE)

colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')

set.seed(100)
train.id = sample(nrow(ratings), floor(nrow(ratings)) * 0.8)
train = ratings[train.id, ]
 
test = ratings[-train.id, ]



i = paste0('u', train$UserID)
j = paste0('m', train$MovieID)
x = train$Rating
tmp = data.frame(i, j, x, stringsAsFactors = T)
Rmat = sparseMatrix(as.integer(tmp$i), as.integer(tmp$j), x = tmp$x)
rownames(Rmat) = levels(tmp$i)
colnames(Rmat) = levels(tmp$j)
Rmat = new('realRatingMatrix', data = Rmat)

            
nsplits = 10

es = evaluationScheme(Rmat, method='split', train=0.8, k=nsplits, given=3)
error_IBCF = matrix(NA, nrow=10, ncol = 3)
error_UBCF = matrix(NA, nrow=10, ncol = 3)

for ( i in 1:nsplits){
  train = getData(es, 'train', run=i)
  test_known = getData(es, 'known', run=i)
  test_unknown = getData(es, 'unknown', run=i)
  # Train recommender.
  rec_IBCF = Recommender(train, method = 'IBCF', parameter = list(normalize = 'z-score', method = 'cosine', nn = 25))
  rec_UBCF = Recommender(train, method = 'UBCF', parameter = list(normalize = 'z-score', method = 'cosine', nn = 25))
  # Predict on test.
  p_IBCF = predict(rec_IBCF, test_known, type = "ratings")
  p_UBCF = predict(rec_UBCF, test_known, type = "ratings")
  
  # Evaluate accuracy.
  error_IBCF[i,] = calcPredictionAccuracy(p_IBCF, test_unknown)
  error_UBCF[i,] = calcPredictionAccuracy(p_UBCF, test_unknown)
  
  
}



```

```{r}
print(error_IBCF)
print(error_UBCF)
```

Error measures of the above run of algorithms:

```{r}
##### UBCF
error_UBCF$id = 1:nrow(error_UBCF)
melted_error_UBCF <- melt(error_UBCF, id='id')
names(melted_error_UBCF) <- c('id', 'measure', 'value')
ggplot() + geom_line(data = melted_error_UBCF, aes(x = id, y = value, color = measure, group = measure)) + scale_x_continuous(breaks = 1:10)
```
##### IBCF

```{r}

error_IBCF$id = 1:nrow(error_IBCF)
melted_error_IBCF <- melt(error_IBCF, id='id')
names(melted_error_IBCF) <- c('id', 'measure', 'value')
ggplot() + geom_line(data = melted_error_IBCF, aes(x = id, y = value, color = measure, group = measure), size = 1) + scale_x_continuous(breaks = 1:10)
```

```



```{r}
library(knitr)
library(kableExtra)
results = as.data.frame(matrix(NA, ncol = 3, nrows = 10))

names(results) = c("Iteration_Number", "UBCF_RMSE", "IBCF_RMSE")
results$Iteration_Number = seq(1,10)
results$UBCF_RMSE = error_UBCF[,1]
results$IBCF_RMSE = error_IBCF[,1]



results %>%
  kbl(caption = "Comparison of RMSE of UBCF and IBCF") %>%
  kable_classic(full_width = F, html_font = "Cambria")


```

